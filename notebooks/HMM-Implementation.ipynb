{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section A: Training a Hidden Markov Model (20 Marks)\n",
    "\n",
    "In this part of the assignment you have to train a Hidden Markov Model (HMM) for part-of-speech (POS)\n",
    "tagging. Look at the solutions from Lab 3, Exercise 3 and Exercise 4 as a reminder for what you have to\n",
    "compute.\n",
    "\n",
    "You will need to create and train two models—an Emission Model and a Transition Model as described in\n",
    "lectures.\n",
    "\n",
    "Use labelled sentences from the ‘news’ part of the Brown corpus. These are annotated with parts of speech,\n",
    "which you will convert into the Universal POS tagset (NLTK uses the smaller version of this set defined by\n",
    "Petrov et al.6). Having a smaller number of labels (states) will make Viterbi decoding faster.\n",
    "Use the last 500 sentences from the corpus as the test set and the rest for training. This split corresponds\n",
    "roughly to a 90/10% division. Do not shuffle the data before splitting.\n",
    "\n",
    "Failure to follow these instructions exactly will render most of your answers incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect, sys, hashlib\n",
    "\n",
    "# Hack around a warning message deep inside scikit learn, loaded by nltk :-(\n",
    "#  Modelled on https://stackoverflow.com/a/25067818\n",
    "import warnings\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    save_filters=warnings.filters\n",
    "    warnings.resetwarnings()\n",
    "    warnings.simplefilter('ignore')\n",
    "    import nltk\n",
    "    warnings.filters=save_filters\n",
    "try:\n",
    "    nltk\n",
    "except NameError:\n",
    "    # didn't load, produce the warning\n",
    "    import nltk\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import map_tag, tagset_mapping\n",
    "\n",
    "################## MY IMPORTS\n",
    "# Modules for computing Frequency Distributions and Probability Distributions\n",
    "from nltk.probability import FreqDist, LidstoneProbDist, ConditionalProbDist\n",
    "\n",
    "# Module for efficient iterating and looping\n",
    "import itertools\n",
    "##################\n",
    "\n",
    "if map_tag('brown', 'universal', 'NR-TL') != 'NOUN':\n",
    "    # Out-of-date tagset, we add a few that we need\n",
    "    tm=tagset_mapping('en-brown','universal')\n",
    "    tm['NR-TL']=tm['NR-TL-HL']='NOUN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (10 Marks)\n",
    "\n",
    "Estimate the Emission model: Fill in the emission_model method of the HMM class. Use a ConditionalProbDist\n",
    "with a LidstoneProbDist estimator.\n",
    "\n",
    "Review the help text for the ConditionalProbDist class. Note that the probdist_factory argument to its\n",
    "__init__ method can be a function that takes a frequency distribution and returns a smoothed probability\n",
    "distribution based on it, a.k.a. an estimator.\n",
    "\n",
    "Review the help text for the LidstoneProbDist class and look particularly at the arguments to the __init__\n",
    "method. You should implement a function to pass to ConditionalProbDist which creates and returns a\n",
    "LidstoneProbDist based on the input frequency distribution, using +0.01 for smoothing and adding an extra\n",
    "bin.\n",
    "\n",
    "Convert all the observations (words) to lowercase.\n",
    "Store the result in the variable self.emission_PD. Save the states (POS tags) that were seen in training in\n",
    "the variable self.states. Both these variables will be used by the Viterbi algorithm in Section B.\n",
    "Define an access function elprob for the emission model by filling in the function template provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (10 Marks)\n",
    "Estimate the Transition model. Fill in the transition_model method of the HMM class. Use a ConditionalProbDist\n",
    "with a LidstoneProbDist estimator using the same approach as in Question 1.\n",
    "\n",
    "When using the training data for this step, add a start token (<s>,<s>) and an end token (</s>,</s>) to\n",
    "each sentence, so that the resulting matrix has useful transition probabilities for transitions from <s> to the real POS tags and from the real POS tags to </s>. For example, for this sentence from the training data:\n",
    "\n",
    "[( ’ Ask ’, ’ VERB ’) , ( ’ jail ’, ’NOUN ’) , (’ deputies ’, ’NOUN ’)]\n",
    "you would use this as part of creating the transition model:\n",
    "\n",
    "[( <s > ,<s >) ,( ’ Ask ’, ’ VERB ’) , ( ’ jail ’, ’NOUN ’) , (’ deputies ’, ’NOUN ’) ,\n",
    "( </s > , </s >)]\n",
    "\n",
    "Store the model in the variable self.transition_PD. This variable will be used by the Viterbi algorithm in\n",
    "Section B.\n",
    "\n",
    "Define an access function tlprob for the transition model by filling in the function template provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section B: Implementing The Viterbi Algorithm (55 Marks)\n",
    "\n",
    "In this part of the assignment you have to implement the Viterbi algorithm. The pseudo-code of the algorithm\n",
    "can be found in the Jurafsky & Martin 3rd edition book in Appendix A7 Figure A.9 in section A.4: use it as a\n",
    "guide for your implementation.\n",
    "\n",
    "However you will need to add to J&M’s algorithm code to make use of the transition probabilities to </s>\n",
    "which are also now in the a matrix.\n",
    "\n",
    "In the pseudo-code the b probabilities correspond to the emission model implemented in part A, question 1\n",
    "and the a probabilities correspond to the transition model implemented in part A, question 2. You should use\n",
    "costs (negative log probabilities). Therefore instead of multiplication of probabilities (as in the pseudo-code)\n",
    "you will do addition of costs, and instead of max and argmax you will use min and argmin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    def __init__(self, train_data, test_data):\n",
    "        \"\"\"\n",
    "        Initialise a new instance of the HMM.\n",
    "\n",
    "        :param train_data: The training dataset, a list of sentences with tags\n",
    "        :type train_data: list(list(tuple(str,str)))\n",
    "        :param test_data: the test/evaluation dataset, a list of sentence with tags\n",
    "        :type test_data: list(list(tuple(str,str)))\n",
    "        \"\"\"\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "\n",
    "        # Emission and transition probability distributions\n",
    "        self.emission_PD = None\n",
    "        self.transition_PD = None\n",
    "        self.states = []\n",
    "\n",
    "        self.viterbi = []\n",
    "        self.backpointer = []\n",
    "\n",
    "    # Compute emission model using ConditionalProbDist with a LidstoneProbDist estimator.\n",
    "    #   To achieve the latter, pass a function\n",
    "    #    as the probdist_factory argument to ConditionalProbDist.\n",
    "    #   This function should take 3 arguments\n",
    "    #    and return a LidstoneProbDist initialised with +0.01 as gamma and an extra bin.\n",
    "    #   See the documentation/help for ConditionalProbDist to see what arguments the\n",
    "    #    probdist_factory function is called with.\n",
    "    def emission_model(self, train_data):\n",
    "        \"\"\"\n",
    "        Compute an emission model using a ConditionalProbDist.\n",
    "\n",
    "        :param train_data: The training dataset, a list of sentences with tags\n",
    "        :type train_data: list(list(tuple(str,str)))\n",
    "        :return: The emission probability distribution and a list of the states\n",
    "        :rtype: Tuple[ConditionalProbDist, list(str)]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('HMM.emission_model')\n",
    "        # TODO prepare data\n",
    "\n",
    "        # Don't forget to lowercase the observation otherwise it mismatches the test data\n",
    "        # Do NOT add <s> or </s> to the input sentences\n",
    "        data = [(tag, word.lower()) for (word, tag) in train_data]\n",
    "\n",
    "        # TODO compute the emission model\n",
    "        emission_FD = FreqDist(data)\n",
    "        lidstone_est = lambda fdist, gamma, bins: nltk.probability.LaplaceProbDist(fdist, 0.01, bins+1)\n",
    "        self.emission_PD = ConditionalProbDist(cfdist=emission_FD, probdist_factory=est)\n",
    "        self.states = list(set([tag for (tag, _) in data]))\n",
    "\n",
    "        return self.emission_PD, self.states\n",
    "\n",
    "    # Access function for testing the emission model\n",
    "    # For example model.elprob('VERB','is') might be -1.4\n",
    "    def elprob(self,state,word):\n",
    "        \"\"\"\n",
    "        The log of the estimated probability of emitting a word from a state\n",
    "\n",
    "        :param state: the state name\n",
    "        :type state: str\n",
    "        :param word: the word\n",
    "        :type word: str\n",
    "        :return: log base 2 of the estimated emission probability\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        #raise NotImplementedError('HMM.elprob')\n",
    "        return emission_PD[state].prob(word)\n",
    "\n",
    "    # Compute transition model using ConditionalProbDist with a LidstonelprobDist estimator.\n",
    "    # See comments for emission_model above for details on the estimator.\n",
    "    def transition_model(self, train_data):\n",
    "        \"\"\"\n",
    "        Compute an transition model using a ConditionalProbDist.\n",
    "\n",
    "        :param train_data: The training dataset, a list of sentences with tags\n",
    "        :type train_data: list(list(tuple(str,str)))\n",
    "        :return: The transition probability distribution\n",
    "        :rtype: ConditionalProbDist\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('HMM.transition_model')\n",
    "        \n",
    "        # TODO: prepare the data\n",
    "        # The data object should be an array of tuples of conditions and observations,\n",
    "        # in our case the tuples will be of the form (tag_(i),tag_(i+1)).\n",
    "        # DON'T FORGET TO ADD THE START SYMBOL </s> and the END SYMBOL </s>\n",
    "        tags_of_sents = [[('<s>',sent[0][1])] + zip(sent[:-1][1], sent[1:][1]) + [(sent[-1][1],'</s>')] for sent in train_data]\n",
    "        data = itertools.chain.from_iterable(tags_of_sents)\n",
    "        \n",
    "        # TODO compute the transition model\n",
    "        transition_FD = FreqDist(data)\n",
    "        lidstone_est = lambda fdist, gamma, bins: nltk.probability.LaplaceProbDist(fdist, 0.01, bins+1)\n",
    "        self.transition_PD = ConditionalProbDist(cfdist=transition_FD, probdist_factory=est)\n",
    "\n",
    "        return self.transition_PD\n",
    "\n",
    "    # Access function for testing the transition model\n",
    "    # For example model.tlprob('VERB','VERB') might be -2.4\n",
    "    def tlprob(self,state1,state2):\n",
    "        \"\"\"\n",
    "        The log of the estimated probability of a transition from one state to another\n",
    "\n",
    "        :param state1: the first state name\n",
    "        :type state1: str\n",
    "        :param state2: the second state name\n",
    "        :type state2: str\n",
    "        :return: log base 2 of the estimated transition probability\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('HMM.tlprob')\n",
    "        return transition_PD[state1].prob(state2)\n",
    "\n",
    "    # Train the HMM\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the HMM from the training data\n",
    "        \"\"\"\n",
    "        self.emission_model(self.train_data)\n",
    "        self.transition_model(self.train_data)\n",
    "\n",
    "    # Part B: Implementing the Viterbi algorithm.\n",
    "\n",
    "    # Initialise data structures for tagging a new sentence.\n",
    "    # Describe the data structures with comments.\n",
    "    # Use the models stored in the variables: self.emission_PD and self.transition_PD\n",
    "    # Input: first word in the sentence to tag\n",
    "    def initialise(self, observation):\n",
    "        \"\"\"\n",
    "        Initialise data structures for tagging a new sentence.\n",
    "\n",
    "        :param observation: the first word in the sentence to tag\n",
    "        :type observation: str\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('HMM.initialise')\n",
    "        # Initialise step 0 of viterbi, including\n",
    "        #  transition from <s> to observation\n",
    "        # use costs (-log-base-2 probabilities)\n",
    "        # TODO\n",
    "\n",
    "        # Initialise step 0 of backpointer\n",
    "        # TODO\n",
    "\n",
    "    # Tag a new sentence using the trained model and already initialised data structures.\n",
    "    # Use the models stored in the variables: self.emission_PD and self.transition_PD.\n",
    "    # Update the self.viterbi and self.backpointer datastructures.\n",
    "    # Describe your implementation with comments.\n",
    "    def tag(self, observations):\n",
    "        \"\"\"\n",
    "        Tag a new sentence using the trained model and already initialised data structures.\n",
    "\n",
    "        :param observations: List of words (a sentence) to be tagged\n",
    "        :type observations: list(str)\n",
    "        :return: List of tags corresponding to each word of the input\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('HMM.tag')\n",
    "        tags = []\n",
    "\n",
    "        for t in ...: # fixme to iterate over steps\n",
    "            for s in ...: # fixme to iterate over states\n",
    "                pass # fixme to update the viterbi and backpointer data structures\n",
    "                #  Use costs, not probabilities\n",
    "\n",
    "        # TODO\n",
    "        # Add a termination step with cost based solely on cost of transition to </s> , end of sentence.\n",
    "\n",
    "        # TODO\n",
    "        # Reconstruct the tag sequence using the backpointer list.\n",
    "        # Return the tag sequence corresponding to the best path as a list.\n",
    "        # The order should match that of the words in the sentence.\n",
    "        tags = ... # fixme\n",
    "\n",
    "        return tags\n",
    "\n",
    "    # Access function for testing the viterbi data structure\n",
    "    # For example model.get_viterbi_value('VERB',2) might be 6.42 \n",
    "    def get_viterbi_value(self, state, step):\n",
    "        \"\"\"\n",
    "        Return the current value from self.viterbi for\n",
    "        the state (tag) at a given step\n",
    "\n",
    "        :param state: A tag name\n",
    "        :type state: str\n",
    "        :param step: The (0-origin) number of a step:  if negative,\n",
    "          counting backwards from the end, i.e. -1 means the last step\n",
    "        :type step: int\n",
    "        :return: The value (a cost) for state as of step\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('HMM.get_viterbi_value')\n",
    "        return ... # fix me\n",
    "\n",
    "    # Access function for testing the backpointer data structure\n",
    "    # For example model.get_backpointer_value('VERB',2) might be 'NOUN'\n",
    "    def get_backpointer_value(self, state, step):\n",
    "        \"\"\"\n",
    "        Return the current backpointer from self.backpointer for\n",
    "        the state (tag) at a given step\n",
    "\n",
    "        :param state: A tag name\n",
    "        :type state: str\n",
    "        :param step: The (0-origin) number of a step:  if negative,\n",
    "          counting backwards from the end, i.e. -1 means the last step\n",
    "        :type step: int\n",
    "        :return: The state name to go back to at step-1\n",
    "        :rtype: str\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('HMM.get_backpointer_value')\n",
    "        return ... # fix me\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question4b():\n",
    "    \"\"\"\n",
    "    Report a hand-chosen tagged sequence that is incorrect, correct it\n",
    "    and discuss\n",
    "    :rtype: list(tuple(str,str)), list(tuple(str,str)), str\n",
    "    :return: your answer [max 280 chars]\n",
    "    \"\"\"\n",
    "    raise NotImplementedError('answer_question4b')\n",
    "\n",
    "    # One sentence, i.e. a list of word/tag pairs, in two versions\n",
    "    #  1) As tagged by your HMM\n",
    "    #  2) With wrong tags corrected by hand\n",
    "    tagged_sequence = 'fixme'\n",
    "    correct_sequence = 'fixme'\n",
    "    # Why do you think the tagger tagged this example incorrectly?\n",
    "    answer =  inspect.cleandoc(\"\"\"\\\n",
    "    fill me in\"\"\")[0:280]\n",
    "\n",
    "    return tagged_sequence, correct_sequence, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question5():\n",
    "    \"\"\"\n",
    "    Suppose you have a hand-crafted grammar that has 100% coverage on\n",
    "        constructions but less than 100% lexical coverage.\n",
    "        How could you use a POS tagger to ensure that the grammar\n",
    "        produces a parse for any well-formed sentence,\n",
    "        even when it doesn't recognise the words within that sentence?\n",
    "\n",
    "    :rtype: str\n",
    "    :return: your answer [max 500 chars]\n",
    "    \"\"\"\n",
    "    raise NotImplementedError('answer_question5')\n",
    "\n",
    "    return inspect.cleandoc(\"\"\"\\\n",
    "    fill me in\"\"\")[0:500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question6():\n",
    "    \"\"\"\n",
    "    Why else, besides the speedup already mentioned above, do you think we\n",
    "    converted the original Brown Corpus tagset to the Universal tagset?\n",
    "    What do you predict would happen if we hadn't done that?  Why?\n",
    "\n",
    "    :rtype: str\n",
    "    :return: your answer [max 500 chars]\n",
    "    \"\"\"\n",
    "    raise NotImplementedError('answer_question6')\n",
    "\n",
    "    return inspect.cleandoc(\"\"\"\\\n",
    "    fill me in\"\"\")[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for testing\n",
    "def isclose(a, b, rel_tol=1e-09, abs_tol=0.0):\n",
    "    # http://stackoverflow.com/a/33024979\n",
    "    return abs(a - b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)\n",
    "\n",
    "def answers():\n",
    "    global tagged_sentences_universal, test_data_universal, \\\n",
    "           train_data_universal, model, test_size, train_size, ttags, \\\n",
    "           correct, incorrect, accuracy, \\\n",
    "           good_tags, bad_tags, answer4b, answer5\n",
    "    \n",
    "    # Load the Brown corpus with the Universal tag set.\n",
    "    tagged_sentences_universal = brown.tagged_sents(categories='news', tagset='universal')\n",
    "\n",
    "    # Divide corpus into train and test data.\n",
    "    test_size = 500\n",
    "    train_size = len(tagged_sentences_universal) - test_size # fixme; DONE?\n",
    "\n",
    "    test_data_universal = tagged_sentences_universal[-test_size:] # fixme; DONE?\n",
    "    train_data_universal = tagged_sentences_universal[:train_size] # fixme; DONE?\n",
    "\n",
    "    if hashlib.md5(''.join(map(lambda x:x[0],train_data_universal[0]+train_data_universal[-1]+test_data_universal[0]+test_data_universal[-1])).encode('utf-8')).hexdigest()!='164179b8e679e96b2d7ff7d360b75735':\n",
    "        print('!!!test/train split (%s/%s) incorrect, most of your answers will be wrong hereafter!!!'%(len(train_data_universal),len(test_data_universal)),file=sys.stderr)\n",
    "\n",
    "    # Create instance of HMM class and initialise the training and test sets.\n",
    "    model = HMM(train_data_universal, test_data_universal)\n",
    "\n",
    "    # Train the HMM.\n",
    "    model.train()\n",
    "\n",
    "    # Some preliminary sanity checks\n",
    "    # Use these as a model for other checks\n",
    "    e_sample=model.elprob('VERB','is')\n",
    "    if not (type(e_sample)==float and e_sample<=0.0):\n",
    "        print('elprob value (%s) must be a log probability'%e_sample,file=sys.stderr)\n",
    "\n",
    "    t_sample=model.tlprob('VERB','VERB')\n",
    "    if not (type(t_sample)==float and t_sample<=0.0):\n",
    "           print('tlprob value (%s) must be a log probability'%t_sample,file=sys.stderr)\n",
    "\n",
    "    if not (type(model.states)==list and \\\n",
    "            len(model.states)>0 and \\\n",
    "            type(model.states[0])==str):\n",
    "        print('model.states value (%s) must be a non-empty list of strings'%model.states,file=sys.stderr)\n",
    "\n",
    "    print('states: %s\\n'%model.states)\n",
    "\n",
    "    ######\n",
    "    # Try the model, and test its accuracy [won't do anything useful\n",
    "    #  until you've filled in the tag method\n",
    "    ######\n",
    "    s='the cat in the hat came back'.split()\n",
    "    model.initialise(s[0])\n",
    "    ttags = [] # fixme\n",
    "    print(\"Tagged a trial sentence:\\n  %s\"%list(zip(s,ttags)))\n",
    "\n",
    "    v_sample=model.get_viterbi_value('VERB',5)\n",
    "    if not (type(v_sample)==float and 0.0<=v_sample):\n",
    "           print('viterbi value (%s) must be a cost'%v_sample,file=sys.stderr)\n",
    "\n",
    "    b_sample=model.get_backpointer_value('VERB',5)\n",
    "    if not (type(b_sample)==str and b_sample in model.states):\n",
    "           print('backpointer value (%s) must be a state name'%b_sample,file=sys.stderr)\n",
    "\n",
    "\n",
    "    # check the model's accuracy (% correct) using the test set\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "\n",
    "    for sentence in test_data_universal:\n",
    "        s = [word.lower() for (word, tag) in sentence]\n",
    "        model.initialise(s[0])\n",
    "        tags = model.tag(s)\n",
    "\n",
    "        for ((word,gold),tag) in zip(sentence,tags):\n",
    "            if tag == gold:\n",
    "                pass # fix me\n",
    "            else:\n",
    "                pass # fix me\n",
    "\n",
    "    accuracy = 0.0 # fix me\n",
    "    print('Tagging accuracy for test set of %s sentences: %.4f'%(test_size,accuracy))\n",
    "\n",
    "    # Print answers for 4b, 5 and 6\n",
    "    bad_tags, good_tags, answer4b = answer_question4b()\n",
    "    print('\\nA tagged-by-your-model version of a sentence:')\n",
    "    print(bad_tags)\n",
    "    print('The tagged version of this sentence from the corpus:')\n",
    "    print(good_tags)\n",
    "    print('\\nDiscussion of the difference:')\n",
    "    print(answer4b[:280])\n",
    "    answer5=answer_question5()\n",
    "    print('\\nFor Q5:')\n",
    "    print(answer5[:500])\n",
    "    answer6=answer_question6()\n",
    "    print('\\nFor Q6:')\n",
    "    print(answer6[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sys.argv)>1 and sys.argv[1] == '--answers':\n",
    "        import adrive2_embed\n",
    "        from autodrive_embed import run, carefulBind\n",
    "        with open(\"userErrs.txt\",\"w\") as errlog:\n",
    "            run(globals(),answers,adrive2_embed.a2answers,errlog)\n",
    "    else:\n",
    "        answers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
